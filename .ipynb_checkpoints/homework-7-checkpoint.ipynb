{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d936a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2DTranspose\n",
    "from numpy import expand_dims\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from keras.datasets.mnist import load_data\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "\n",
    "#data to be used for the discriminator\n",
    "(train_img, _), (_, _) = load_data()\n",
    "X = np.expand_dims(train_img, axis=-1)\n",
    "X = X.astype('float32')\n",
    "X = X / 255.0\n",
    "\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95d7c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a random subset of the training data and creates a dataest with \"true\" (1) labels\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "#this function generates random images and creates a dataet of size n_samples with \"fake\" (0) labels\n",
    "def generate_fake_samples(n_samples):\n",
    "    # generate uniform random numbers in [0,1]\n",
    "    X = rand(28 * 28 * n_samples)\n",
    "    # reshape into a batch of grayscale images\n",
    "    X = X.reshape((n_samples, 28, 28, 1))\n",
    "    # generate 'fake' class labels (0)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    "# train the discriminator model\n",
    "def train_discriminator(model, dataset, n_iter=100, n_batch=256):\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_iter):\n",
    "        # get randomly selected 'real' samples\n",
    "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "        # update discriminator on real samples\n",
    "        _, real_acc = model.train_on_batch(X_real, y_real)\n",
    "        # generate 'fake' examples\n",
    "        X_fake, y_fake = generate_fake_samples(half_batch)\n",
    "        # update discriminator on fake samples\n",
    "        _, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
    "        # summarize performance\n",
    "        print('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c60e9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 real=65% fake=45%\n",
      ">2 real=73% fake=57%\n",
      ">3 real=72% fake=80%\n",
      ">4 real=73% fake=95%\n",
      ">5 real=75% fake=96%\n",
      ">6 real=72% fake=100%\n",
      ">7 real=73% fake=99%\n",
      ">8 real=79% fake=100%\n",
      ">9 real=73% fake=100%\n",
      ">10 real=73% fake=100%\n",
      ">11 real=78% fake=100%\n",
      ">12 real=76% fake=100%\n",
      ">13 real=77% fake=100%\n",
      ">14 real=75% fake=100%\n",
      ">15 real=88% fake=100%\n",
      ">16 real=82% fake=100%\n",
      ">17 real=81% fake=100%\n",
      ">18 real=86% fake=100%\n",
      ">19 real=88% fake=100%\n",
      ">20 real=91% fake=100%\n",
      ">21 real=92% fake=100%\n",
      ">22 real=97% fake=100%\n",
      ">23 real=95% fake=100%\n",
      ">24 real=96% fake=100%\n",
      ">25 real=97% fake=100%\n",
      ">26 real=98% fake=100%\n",
      ">27 real=100% fake=100%\n",
      ">28 real=99% fake=100%\n",
      ">29 real=99% fake=100%\n",
      ">30 real=100% fake=100%\n",
      ">31 real=98% fake=100%\n",
      ">32 real=100% fake=100%\n",
      ">33 real=100% fake=100%\n",
      ">34 real=100% fake=100%\n",
      ">35 real=100% fake=100%\n",
      ">36 real=99% fake=100%\n",
      ">37 real=100% fake=100%\n",
      ">38 real=100% fake=100%\n",
      ">39 real=100% fake=100%\n",
      ">40 real=100% fake=100%\n",
      ">41 real=100% fake=100%\n",
      ">42 real=99% fake=100%\n",
      ">43 real=100% fake=100%\n",
      ">44 real=100% fake=100%\n",
      ">45 real=100% fake=100%\n",
      ">46 real=100% fake=100%\n",
      ">47 real=100% fake=100%\n",
      ">48 real=100% fake=100%\n",
      ">49 real=100% fake=100%\n",
      ">50 real=100% fake=100%\n",
      ">51 real=100% fake=100%\n",
      ">52 real=100% fake=100%\n",
      ">53 real=100% fake=100%\n",
      ">54 real=100% fake=100%\n",
      ">55 real=100% fake=100%\n",
      ">56 real=100% fake=100%\n",
      ">57 real=100% fake=100%\n",
      ">58 real=100% fake=100%\n",
      ">59 real=100% fake=100%\n",
      ">60 real=100% fake=100%\n",
      ">61 real=100% fake=100%\n",
      ">62 real=100% fake=100%\n",
      ">63 real=100% fake=100%\n",
      ">64 real=100% fake=100%\n",
      ">65 real=100% fake=100%\n",
      ">66 real=100% fake=100%\n",
      ">67 real=100% fake=100%\n",
      ">68 real=100% fake=100%\n",
      ">69 real=100% fake=100%\n",
      ">70 real=100% fake=100%\n",
      ">71 real=100% fake=100%\n",
      ">72 real=100% fake=100%\n",
      ">73 real=100% fake=100%\n",
      ">74 real=100% fake=100%\n",
      ">75 real=100% fake=100%\n",
      ">76 real=100% fake=100%\n",
      ">77 real=100% fake=100%\n",
      ">78 real=100% fake=100%\n",
      ">79 real=100% fake=100%\n",
      ">80 real=100% fake=100%\n",
      ">81 real=100% fake=100%\n",
      ">82 real=100% fake=100%\n",
      ">83 real=100% fake=100%\n",
      ">84 real=100% fake=100%\n",
      ">85 real=100% fake=100%\n",
      ">86 real=100% fake=100%\n",
      ">87 real=100% fake=100%\n",
      ">88 real=100% fake=100%\n",
      ">89 real=100% fake=100%\n",
      ">90 real=100% fake=100%\n",
      ">91 real=100% fake=100%\n",
      ">92 real=100% fake=100%\n",
      ">93 real=100% fake=100%\n",
      ">94 real=100% fake=100%\n",
      ">95 real=100% fake=100%\n",
      ">96 real=100% fake=100%\n",
      ">97 real=100% fake=100%\n",
      ">98 real=100% fake=100%\n",
      ">99 real=100% fake=100%\n",
      ">100 real=100% fake=100%\n"
     ]
    }
   ],
   "source": [
    "# define the discriminator model\n",
    "dis_model = define_discriminator()\n",
    "# fit the model\n",
    "train_discriminator(dis_model, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bb12bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)   (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 28, 28, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)   (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 28, 28, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 1,164,289\n",
      "Trainable params: 1,164,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model\n",
    "\n",
    "# define the size of the latent space\n",
    "latent_dim = 100\n",
    "# define the generator model\n",
    "gen_model = define_generator(latent_dim)\n",
    "# summarize the model\n",
    "gen_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "566eb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0408cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_24 (Sequential)   (None, 28, 28, 1)         1164289   \n",
      "_________________________________________________________________\n",
      "sequential_23 (Sequential)   (None, 1)                 40705     \n",
      "=================================================================\n",
      "Total params: 1,204,994\n",
      "Trainable params: 1,164,289\n",
      "Non-trainable params: 40,705\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_model = define_gan(gen_model, dis_model)\n",
    "# summarize gan model\n",
    "gan_model.summary()\n",
    "def train_gan(gan_model, latent_dim, n_epochs=100, n_batch=256):\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        gan_model.train_on_batch(x_gan, y_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5d6122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # create training set for the discriminator\n",
    "            X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
    "            # update discriminator model weights\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "            \n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save plot\n",
    "    save_plot(x_fake, epoch)\n",
    "    # save the generator model tile file\n",
    "    filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(filename)\n",
    "    \n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # evaluate the model performance, sometimes\n",
    "        if (i+1) % 10 == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
    "            \n",
    "# save the generator model tile file\n",
    "filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "g_model.save(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
