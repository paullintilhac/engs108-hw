{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d936a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.datasets.mnist import load_data\n",
    "import numpy as np\n",
    "\n",
    "# Note: I based my code largely on the tutorial that is available here:\n",
    "# https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "# and then adapted it to the specific assignment, including modifying it to create a separate GAN for each digit,\n",
    "# changing the hyperparameters, and adding new plotting functionality. I made sure to read through all the \n",
    "# explanatory text while walking through the tutorial to make sure I knew what each part was doing, \n",
    "# and why it was structured the way it was.\n",
    "\n",
    "\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# train the discriminator model\n",
    "def train_discriminator(model, dataset, n_iter=100, batch_size=256):\n",
    "    half_batch = int(batch_size / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_iter):\n",
    "        # get randomly selected 'real' samples\n",
    "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "        # update discriminator on real samples\n",
    "        _, real_acc = model.train_on_batch(X_real, y_real)\n",
    "        # generate 'fake' examples\n",
    "        X_fake, y_fake = generate_fake_samples(half_batch)\n",
    "        # update discriminator on fake samples\n",
    "        _, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
    "        # summarize performance\n",
    "        print('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d7c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a random subset of the training data and creates a dataest with \"true\" (1) labels\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "#this function generates random images and creates a dataet of size n_samples with \"fake\" (0) labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# load and prepare mnist training images\n",
    "def load_real_samples(digit):\n",
    "    # load mnist dataset\n",
    "    (trainX, trainY), (_, _) = load_data()\n",
    "    #print(\"head of train_X\")\n",
    "    #print(str(trainX[:5]))\n",
    "    train_ind = np.where(trainY==digit)\n",
    "    trainX = trainX[train_ind]\n",
    "    # expand to 3d, e.g. add channels dimension\n",
    "    X = np.expand_dims(trainX, axis=-1)\n",
    "    # convert from unsigned ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [0,1]\n",
    "    X = X / 255.0\n",
    "    return X\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb12bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566eb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def train_gan(gan_model, latent_dim, n_epochs=100, batch_size=256):\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, batch_size)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = np.ones((batch_size, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        gan_model.train_on_batch(x_gan, y_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5d6122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, digit,n_samples=100):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save the generator model tile file\n",
    "    filename = str(digit)+'_ld'+str(latent_dim)+'_generator_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(filename)\n",
    "    \n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, digit, n_epochs=20, batch_size=256,):\n",
    "    bat_per_epo = int(dataset.shape[0] / batch_size)\n",
    "    half_batch = int(batch_size / 2)\n",
    "    # manually enumerate epochs\n",
    "    print(\"training GAN model for digit: \" + str(digit))\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # create training set for the discriminator\n",
    "            X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "            # update discriminator model weights\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, batch_size)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = np.ones((batch_size, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "        # evaluate the model performance, sometimes\n",
    "        if (i+1) % 5 == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim,digit)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14e29186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training GAN model for digit: 4\n",
      "WARNING:tensorflow:5 out of the last 888 calls to <function Model.make_train_function.<locals>.train_function at 0x7fac68f87040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      ">1, 1/22, d=0.701, g=0.678\n",
      ">1, 2/22, d=0.697, g=0.700\n",
      ">1, 3/22, d=0.691, g=0.719\n",
      ">1, 4/22, d=0.680, g=0.736\n",
      ">1, 5/22, d=0.670, g=0.748\n",
      ">1, 6/22, d=0.663, g=0.773\n",
      ">1, 7/22, d=0.655, g=0.798\n",
      ">1, 8/22, d=0.647, g=0.818\n",
      ">1, 9/22, d=0.642, g=0.831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3dae02350bce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_real_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-f284091c81c5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(g_model, d_model, gan_model, dataset, latent_dim, digit, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0my_gan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# update the generator via the discriminator's error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_gan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m# summarize loss on this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>%d, %d/%d, d=%.3f, g=%.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbat_per_epo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1854\u001b[0m                                                     class_weight)\n\u001b[1;32m   1855\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # size of the latent space\n",
    "    latent_dim = 5\n",
    "    # create the discriminator\n",
    "    d_model = define_discriminator()\n",
    "    # create the generator\n",
    "    g_model = define_generator(latent_dim)\n",
    "    # create the gan\n",
    "    gan_model = define_gan(g_model, d_model)\n",
    "    # load image data\n",
    "    dataset = load_real_samples(i)\n",
    "    # train model\n",
    "    train(g_model, d_model, gan_model, dataset, latent_dim,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "160db6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from matplotlib import pyplot\n",
    "def split(word):\n",
    "    return [char for char in word]\n",
    "\n",
    "def generate_png(digits_string):\n",
    "    digit_list = split(digits_string)\n",
    "    n = len(digit_list)\n",
    "    print(\"n: \" + str(n))\n",
    "    count =0\n",
    "    for digit in digit_list:\n",
    "        count=count+1\n",
    "        model = load_model('/Users/paullintilhac/engs108-hw/'+digit+'_'+'generator_model_010.h5')\n",
    "        latent_points = generate_latent_points(10, 1)\n",
    "        X = model.predict(latent_points)\n",
    "        pyplot.subplot(1, n, count)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(X[0, :, :, 0], cmap='gray_r')\n",
    "        pyplot.show()\n",
    "\n",
    "generate_png(\"4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
