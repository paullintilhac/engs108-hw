{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d936a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.datasets.mnist import load_data\n",
    "import numpy as np\n",
    "\n",
    "# Note: I based my code largely on the tutorial that is available here:\n",
    "# https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "# and then adapted it to the specific assignment, including modifying it to create a separate GAN for each digit,\n",
    "# changing the hyperparameters, and adding new plotting functionality. I made sure to read through all the \n",
    "# explanatory text while walking through the tutorial to make sure I knew what each part was doing, \n",
    "# and why it was structured the way it was.\n",
    "\n",
    "\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# train the discriminator model\n",
    "def train_discriminator(model, dataset, n_iter=100, batch_size=256):\n",
    "    half_batch = int(batch_size / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_iter):\n",
    "        # get randomly selected 'real' samples\n",
    "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "        # update discriminator on real samples\n",
    "        _, real_acc = model.train_on_batch(X_real, y_real)\n",
    "        # generate 'fake' examples\n",
    "        X_fake, y_fake = generate_fake_samples(half_batch)\n",
    "        # update discriminator on fake samples\n",
    "        _, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
    "        # summarize performance\n",
    "        print('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d7c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a random subset of the training data and creates a dataest with \"true\" (1) labels\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "#this function generates random images and creates a dataet of size n_samples with \"fake\" (0) labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# load and prepare mnist training images\n",
    "def load_real_samples(digit):\n",
    "    # load mnist dataset\n",
    "    (trainX, trainY), (_, _) = load_data()\n",
    "    #print(\"head of train_X\")\n",
    "    #print(str(trainX[:5]))\n",
    "    train_ind = np.where(trainY==digit)\n",
    "    trainX = trainX[train_ind]\n",
    "    # expand to 3d, e.g. add channels dimension\n",
    "    X = np.expand_dims(trainX, axis=-1)\n",
    "    # convert from unsigned ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [0,1]\n",
    "    X = X / 255.0\n",
    "    return X\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb12bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566eb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def train_gan(gan_model, latent_dim, n_epochs=100, batch_size=256):\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, batch_size)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = np.ones((batch_size, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        gan_model.train_on_batch(x_gan, y_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5d6122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, digit,n_samples=100):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save the generator model tile file\n",
    "    filename = str(digit)+'_ld'+str(latent_dim)+'_generator_model_%03d.h5' % (epoch + 1)\n",
    "    g_model.save(filename)\n",
    "    \n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, digit, n_epochs=10, batch_size=256,):\n",
    "    bat_per_epo = int(dataset.shape[0] / batch_size)\n",
    "    half_batch = int(batch_size / 2)\n",
    "    # manually enumerate epochs\n",
    "    print(\"training GAN model for digit: \" + str(digit))\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # create training set for the discriminator\n",
    "            X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "            # update discriminator model weights\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, batch_size)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = np.ones((batch_size, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "        # evaluate the model performance, sometimes\n",
    "        if (i+1) % 5 == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim,digit)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e29186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training GAN model for digit: 3\n",
      ">1, 1/23, d=0.682, g=0.736\n",
      ">1, 2/23, d=0.675, g=0.753\n",
      ">1, 3/23, d=0.667, g=0.767\n",
      ">1, 4/23, d=0.659, g=0.785\n",
      ">1, 5/23, d=0.648, g=0.805\n",
      ">1, 6/23, d=0.643, g=0.816\n",
      ">1, 7/23, d=0.634, g=0.830\n",
      ">1, 8/23, d=0.623, g=0.855\n",
      ">1, 9/23, d=0.616, g=0.870\n",
      ">1, 10/23, d=0.612, g=0.883\n",
      ">1, 11/23, d=0.600, g=0.897\n",
      ">1, 12/23, d=0.601, g=0.914\n",
      ">1, 13/23, d=0.592, g=0.912\n",
      ">1, 14/23, d=0.590, g=0.913\n",
      ">1, 15/23, d=0.593, g=0.895\n",
      ">1, 16/23, d=0.591, g=0.870\n",
      ">1, 17/23, d=0.597, g=0.842\n",
      ">1, 18/23, d=0.601, g=0.817\n",
      ">1, 19/23, d=0.607, g=0.788\n",
      ">1, 20/23, d=0.604, g=0.769\n",
      ">1, 21/23, d=0.609, g=0.750\n",
      ">1, 22/23, d=0.600, g=0.737\n",
      ">1, 23/23, d=0.593, g=0.730\n",
      ">2, 1/23, d=0.589, g=0.723\n",
      ">2, 2/23, d=0.585, g=0.718\n",
      ">2, 3/23, d=0.580, g=0.717\n",
      ">2, 4/23, d=0.574, g=0.713\n",
      ">2, 5/23, d=0.568, g=0.710\n",
      ">2, 6/23, d=0.554, g=0.710\n",
      ">2, 7/23, d=0.548, g=0.708\n",
      ">2, 8/23, d=0.543, g=0.707\n",
      ">2, 9/23, d=0.526, g=0.705\n",
      ">2, 10/23, d=0.522, g=0.705\n",
      ">2, 11/23, d=0.514, g=0.704\n",
      ">2, 12/23, d=0.501, g=0.704\n",
      ">2, 13/23, d=0.496, g=0.703\n",
      ">2, 14/23, d=0.484, g=0.703\n",
      ">2, 15/23, d=0.479, g=0.701\n",
      ">2, 16/23, d=0.473, g=0.700\n",
      ">2, 17/23, d=0.467, g=0.695\n",
      ">2, 18/23, d=0.472, g=0.688\n",
      ">2, 19/23, d=0.463, g=0.686\n",
      ">2, 20/23, d=0.470, g=0.665\n",
      ">2, 21/23, d=0.474, g=0.643\n",
      ">2, 22/23, d=0.478, g=0.639\n",
      ">2, 23/23, d=0.493, g=0.619\n",
      ">3, 1/23, d=0.499, g=0.590\n",
      ">3, 2/23, d=0.529, g=0.566\n",
      ">3, 3/23, d=0.539, g=0.558\n",
      ">3, 4/23, d=0.548, g=0.541\n",
      ">3, 5/23, d=0.546, g=0.559\n",
      ">3, 6/23, d=0.579, g=0.531\n",
      ">3, 7/23, d=0.584, g=0.548\n",
      ">3, 8/23, d=0.571, g=0.552\n",
      ">3, 9/23, d=0.556, g=0.566\n",
      ">3, 10/23, d=0.554, g=0.586\n",
      ">3, 11/23, d=0.562, g=0.578\n",
      ">3, 12/23, d=0.561, g=0.599\n",
      ">3, 13/23, d=0.555, g=0.611\n",
      ">3, 14/23, d=0.572, g=0.622\n",
      ">3, 15/23, d=0.545, g=0.635\n",
      ">3, 16/23, d=0.543, g=0.634\n",
      ">3, 17/23, d=0.547, g=0.640\n"
     ]
    }
   ],
   "source": [
    "for i in [3,4,5,6,7,8,9]:\n",
    "    # size of the latent space\n",
    "    latent_dim = 5\n",
    "    # create the discriminator\n",
    "    d_model = define_discriminator()\n",
    "    # create the generator\n",
    "    g_model = define_generator(latent_dim)\n",
    "    # create the gan\n",
    "    gan_model = define_gan(g_model, d_model)\n",
    "    # load image data\n",
    "    dataset = load_real_samples(i)\n",
    "    # train model\n",
    "    train(g_model, d_model, gan_model, dataset, latent_dim,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44021c",
   "metadata": {},
   "source": [
    "the code below uses the model that was trained on 5 epochs, using 5 latent dimensions. I found 5 latent dimensions to be better than 10 or 100, which was recommended by the tutorial when using a composite model for all 10 digits. When too many latent dimensions were used, the discriminator accuracy would crash, and the generator accuracy would spike, after a certain number of epochs. The pictures it generated ultimately created a checkerboard pattern. However, it should be noted that the low number of epochs impacts the overall quality in terms of similarity to human digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "160db6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAACiElEQVR4nO2WsUojURSGvxljjMSogSjGRMQiGlAESWUjCCppYmGjPoH4BL6KoiksxMLKpBQx2FgoSNCAsRHFwgxJiCSmcPCeLRZkl3XM1RW0yA+3mpn/u4e55z/XEBG+S+a3kZvwJvxHwbPZrMjvXnRctm1LrVaTSqUiT09Pb73vKKNBn2uFgIhgGIYjw+mBS8fcCVitVrm8vKRcLhMMBhkdHcXj8Wh7fAp+f3/P0dERe3t75PN5lFL4/X6WlpZYXV2lvb1dv4J31j/K5XKyvLwsoVBIenp6JBwOy+DgoPT29srw8LCkUilRSv35iaP/hypXSnF1dUUul8Pn8xGJRAgGg9Trdc7Oznh4eGB3d5fp6Wk6Ojoa+n0IXiwWSafTmKZJIpFgfn6evr4+LMtifX2ddDrN3d0dtm1r+WnDbdtmZ2eHTCbDxMQEi4uLjI+P09LSQn9/P4lEgtPTU2KxGF6v92vhJycnbGxsADA7O0s0GqW1tRUAt9tNKBRiaGiIhYUF3G63lqdWwtVqNZLJJOVymcnJSWZmZl5PtIjw+PjIwcEBpVLpdUM60qrcsizOz8/xer3Mzc0RDocxTROlFMVikf39fTY3N3l+fqZarX4t/OXlhZGREUqlEgMDAxiGgW3bFAoFtra22N7eplAoEAgE9HsczXgVEbLZLIeHh5imic/n4/b2lkwmw8XFBfV6HZfLRTweJ5lM4vf7/2I4u2uGjFJKbm5uZG1tTaLRqHR1dYnH45Hu7m4ZGxuTlZUVub6+fiuXHP0/NFhEBMuyOD4+Jp/P09nZydTUFJFIhLa2NlyuN/+i88T57FRrMMm04J++TGiC31Wj0/7/hHf0c69RTXgT/tX6BTfhqNmeshEAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAr0lEQVR4nO3WMQrEIBAF0G/cxjaQwsYj5bjpPEqwsEphM50gzh5gccOGMGHR3/7i4YDMKGbGU5kekwc+cOm8Tvo7/qFqFf2OfeD/h9daUWuVx5kZ+74jpSSP55yxbRtCCPL4cRzw3oOI5PEYI2KM0FrL4swMIsKyLJjnWRZXSsE5h3VdYa29jKuTG65ZllKQc4YxBtP09Q3NxXIZ/yFjq33k7JhojuyO9Dv2gfeHvwFC1z0uEmenmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAACs0lEQVR4nO2WTUvrQBiFn6SJNSp+xPpZtFAQiwsxuMvKnVt/gLtu/FuC227EXXcFQQTBiiClFQvSBms11hpJYzK5i4uXKlTbIteFnt0ww3nmzLzvMFIYhnyX5G8j/8J/4f9bykeTQohQluX2MUEQ0Gq1aDab2LbN4OAgsViMkZER2teGYUihUCCVSkl9wdvNXseyLKOqKgMDAwC4rguAJL1lSJJEKpX6yP5j+HsJIajX61QqFU5PTzk7O+Pp6YmFhQU2NzdZXV1F07Su/aRPXrg3k/l8nkwmw/7+PpVKBdd1kSQJTdNIJpNsbW2xvb3N/Px8+0l0PPaeCk7XdRYXF1lZWWF5eZlEIsHo6ChCCC4vL9nd3SWXy+F5Xld+PSV/VRAEeJ5HrVbj8PCQbDbL+fk5juOQTqdJp9OMj48ThiHS+2JoU093/qpIJIKmaSQSCaamppidnSWbzWJZFktLS6iq+nfnf+EdffpK/mocBAGNRoNCocD19TW6rrO2tkYsFkOSpK9NLoTAdV0cx8G2barVKuVyGdu2/9WCruv/0n6Uuie4EIJSqcTx8TH5fJ6bmxts28ZxHIaHhzFNE8MwPgX2Bb+7u2Nvb49cLsf9/T2tVgvP8wiCAEVRCIIAwzCIx+NEo9GuPLtqNSEE5XKZo6Mjrq6uaDabAESjUSKRCL7vUywWOTg4wLIsuv0ddZ3ctm1ub2+p1Wqoqsrk5CRzc3OEYUij0eDx8ZFMJsPDwwM7OzuYpokQAkXpjOgKLssy6+vrJJNJ6vU6iqKwsbGBaZrMzMxQrVYpFoucnJzg+z6lUgnDMHAch+np6Y6+PbWaZVlcXFwwNjZGPB5nYmICVVUJw5CXlxeen58RQjA0NISmafi+j6qqHSuw7z7vQV/ztn+1Prvz7pu2D/3cP9wv/OfB/wD/jDgMPUOpiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB/0lEQVR4nO1WvYrqQBg9uSZpY1Ik0U4ECYJg6QOIlTY+gY/hS/giKgTfQBC0EwshhRCwMYQYLPzBxEy+LS542bvqJrt37xbrgTSZ5Jw535z5Zjgiwnfh17cpP8Wf4v8b/KNBIiKO4z6rcZfgofN/IPwQD53fQhAEYIy9JeJ5iKKYjoyIHj2vMJvNqF6vk6ZppKoqaZpGuq5TLpejdrtNnuf9/Qs94k/sPI5jjEYj2LYNWZbB8/y1EpfL5fpNGnDv9PbrYBRFsCwLgiAgn89DFEUwxuC6LjabDYrFInRdv5WTu8FJ7JzneZTL5Ss5YwxhGOJ0OuFwOCCKoqRUf2aV1Pn1BRFs28ZwOMR8PofneQCAVquFTqcDWZbfaNwjT512juNwOp2wXq8xnU5xOBwgCAIYYzAMA41GA5lMJhlXWufAb/fH4xGr1QqTyQTj8RiO46BWq6Hb7ULTtFca99lTbLVbCMOQBoMBVatVKpVKZJomxXGcaKt9qrcTEfb7PZbLJXa7HbLZLAqFQuLOmGrNfd+HZVk4n8/Ybrfo9/uwLAuu60JVVfR6PVQqlXSzT1r2xWJBzWaTVFUlSZJIkiRSFIUMwyDTNCkIglQdLlXgiAhBEMD3fTiOgyiKIEkSVFWFoij3yn13DT6U9pT42JH61XgvcF96oP/cO9xT/OeJvwDY0c1PMMfcYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAACH0lEQVR4nO2WMWvqYBSG38QYFZVikRpwEAdFsIObOBaKq67+Bv+Hu7/BoX/ATdDFwUWcLCgiVbAQYgOBVJN8mtNJufVebSL2dqgvZPnOl/fhnMPhfBwR4afE/xj5Cr/C/7eEU0EiIo7jHBkREYgIPP9XPkcNTsKdgt3e3cl12bfbLRhjn86ICLIs4+XlBZZlOTfblevI90mGYdDT0xPV63WyLGt/vlwuqVKpULFYpMFgcPjbUX9XmS8WC9RqNTSbTby9ve3PZVlGt9vFfD6HruuO/VzBZVmGoijQdR3r9RoAYNs2Xl9fsdlskMvlkMlkLg/fQWzbhiiK+76bponhcAiv14tyuYzb29vLwxljGI1GsG0bpmlCVVVYloXFYoFOpwOO45BOp/81akd1ctT+lKZpmEwmEAQBpmni+fkZkUgErVYLvV4PyWQSsVjMMdgVfNdnv98PABgMBlAUBe12G+/v73h4eEA0Gv0eeCgUwuPjI+7u7mAYBhhj6Pf7mE6nCAaDyOfzEEXRFdzVnDPGaLVa0Ww2o0ajQaVSiSRJomw2S/P5/PD6ZedcEAQEAgHE43EkEgmoqgpN08DzPAKBgLusceZW83g8YIxBURR4PB6kUin4fD7XPo57fihRFHF/f49CoYBqtYpwOOzag/vi9Xo0qKoqxuMxEokEJEk6tdWOBs4qOxGB4zhIkoSbm5uz1inwdebfqt/7hrvCfx/8A++miLYB6o/jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABQUlEQVR4nO2WO67CMBBFjwkQKXQgooSKDbCGiI0gsad07IUaegpa5D5QgFA+ZqheGYKDn6DIlVxZmmPPnRlbiQjfUu9r5A7ewX8KXpalAJ+uWqmGPncxBFTdRv+TqCLC3+F7PXsHW8HP5zO73Y7j8cjtdmM4HLJYLEiShCAI/g9ujGG73bLZbMiyjNlsxnK5JI5j+n27cNae53nO4XDA933m8zmj0Qilam2FF55bwUWEPM+pquodaCPcukqUUnie9y74pazhIkJVVbh4ilv1uYjY3Nxd2gEnKW8NdyWrxhQRiqJAa839fscYg+/7hGHIeDy2zsjb8Mfjwel0Ik1T9vs9WZYhIkwmE9brNavVisFgYAW3KjhjDNfrFa01l8uFKIqYTqcEQfAK7GbItJTbanelJs/d9FSNfvcb1cE7uGs9ATOOlERYUDJ8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAACEklEQVR4nO2WPU8qQRiFn1l23Q1RA4ktIVAoNn70tHQUNPIDCDWNDf+FwsbCzr9hYsxWFiba4QcNGzbIrDO7GatbXXZ1CV5voieZat45T85JdmeEMYbvkvVt5F/4L/xfy87ajOPY2Hb6iDGGxWKBUorNzU0cx1k2JtLOZyb/CByGIb7v4/s+Usosq3STjLVUURSZ6+trMxgMzPHxsTk6OjJnZ2dGa71sPNU/s/Zlenx85Pz8nIuLC+7u7nh7e2Nra4vFYvG1ybXWZjQamXq9bjzPM47jmJ2dHTMYDMx0Ok0raj3J4zjm5eUFpRQbGxsUi0X6/T6np6eUSqXcwXN9akIIHMfBtm2KxSK1Wo1Wq8X29nZucG54kiTM53Msy8LzPFzXZTKZEIYhcRyjtSaKIuI4/pRfrtqllDw9PfHnGg6CgMvLS8bjMeVymfF4TBiGtNttms0mlpWdLRc8CAKen5/RWmOMYTqdcnV1xc3NDVprgiDA8zwODg4QIvXfsho8iiIsy0IIgdaaJElQSqGUIkkSLMtif3+fw8PD9cPr9TrdbpfZbMbDwwNSSpRSAOzu7nJyckKv16NSqXzKLxfc8zyq1SpCCF5fX4miiEKhwN7eHsPhkE6ng+u6n/YTH7zh/tqUUnJ7e8v9/T1SSkqlEo1Gg0ajkVZ1av+54StotVvtq/X/PibIqGwd+rm1/8J/Hvwdx7GGJ7M7clsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+klEQVR4nO2Vu4oiQRSG/9J22sRbY2KgiBcwkVZBUQON+gkMBEMfwMg3EHwD8SkMDQzNDITGzFDBQARNvATa7b/B7Cy7s+vYzjrMwPhBBU0fzldVnFNHkMRnYfs080P+kH8p+XK5JID/XRcRV/r8Ho+AuPTj6177RyNZDdxut5jP51iv1yCJp6cn2Gw2OJ1OkMTpdILdbkckEoHP57OWlORbiyRpGAY7nQ4VRaHD4aDT6aTf72c8HqeqqoxEIvT7/Uyn0xyNRnzFxfyWTn44HDAcDrHf7yGEQDQahaZpUFUVm80Gg8EAuq4jFoshkUhYvUzrcsMwEAgEUCgU0Gg0kEwmIcsylsslFosFZrMZNE2Dx+O5j5wkhBDwer1oNpuw2+1QVRWyLP8Rs9vtkM1mUalULIuvyoV4blFZlpHP5399v2CaJkajEabTKer1uvVC+4nlVnstJgld19HtdiHLMsrl8l8xV7FS7f9iMpmwVCrR7Xaz1WrRMIxLoRfz3yw3TZP9fp+ZTIYOh4Mul4u9Xu+tfd5Hfj6fqes6c7kcJUkiAMZiMY7H43fJLb9wL0iShOPxCEmSoCgK2u020un0rWmec90SLIRAKBRCtVrFarVCrVZDKpWCzfa+EXHzyQ3DQLFYRDAYRDgcvr3Cf+PaPP9Qvu88f8i/n/wH5FEBAHzvBq8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAACJ0lEQVR4nO2WMUsrQRSFv52sYZOQiKI2bmOVShCsbcROi3RiZe9fsLHzJ2iq9AHBJm0Kg9jYiRapBQkhpkmiG+7MtXgP4fGM7i4RLXLgVjNzPw4zZ2Y8VeWnZH6MPIPP4L8KHkWRAv/UJDnn1Dmn+ie7Cujf9RPlfZHzaVwC3qQBP21Hay0ignMOVcUYg+/7+H78lqng/X6fWq1Go9Hg+fkZ5xwLCwvs7u5ydHREoVD4HriIcH19Tb1ep91uY60F4Onpifn5efb3978H3u/3qdfrXF5e0uv1KJVKiAgigqrS7XZpt9uEYYgxXwcpNtxaS6vV4vT0lF6vRzabJZfLISJEUYSI0Ol0GI1Gsc3Ehhtj2Nzc5ODggGazSbfbxTmH53nk83nCMOTw8JCdnZ1YriFF1ESE29tbzs/Pub+/Z3Fxkb29PSqVCmEY4nn/JWt6UTPGEAQB+Xye1dVVtre3PwN/3ivJZGstd3d3nJ2dvbteX19neXk5MRgSOu90OlSrVa6urgiC4P1ySQNOBBcRbm5uaLVaDAYDoiji4eGBi4sLoihiY2ODlZWVZHRV/azeNRwO9fj4WIvFogZBoEEQaKFQ0FKppGtra3pycqKj0eijN2di/9h77vs+W1tbLC0tYa1lPB7z8vLC6+sr2WyWcrnM3NxcIuOJojYej3l8fGQ4HCIiGGPIZDIUi0XCMCSTyXzImAo8pVLnPN0xjqnf+42awWfwaesNZ4RY2W/68iAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB8UlEQVR4nO2WsWoqQRSGv3WjRteVXTYIushCULAQtbHUQrAJCJa+h4Wdb2Cbl0ibQgJJKRYWYivYiIWoICKsG9C5xYVbRePuNZjCH041M/83c2bmzEhCCK4l39XIN/gN/qvg8/lcAP8bRyV9c88vUQSkYw1XTfud14Gfn5+sVits20ZVVQzDwOdztxbXcCEEq9WK5+dnBoMBjuNgmiaVSoVGo8Hd3fmWruBCCJbLJe12m/f3dxRFwe/3M5vN0HWder1OJBI5289VnsbjMa1Wi263i67rWJaFoihst1vm8zn7/d6N3flw27bpdDq8vb0RCoWIxWKEw2Ecx2G/35NMJrm/v788/HA48Pr6Sq/XQ5ZlHh4eiMfjGIaBLMtomkaxWCQYDLqCn7XnkiQRjUZJpVKEQiHy+TzpdJrFYsF0OsU0TbLZrCvw2XCATCZDs9lE0zQ0TcPn8zEcDolEIsTjcRKJxM/AJUnCsiwsywL+nnrbtv8dNMMwXF0xV/CvJrNer3l5eWE0GqGqKl6+Y57K62634+Pjg36/z2azIRAIIMuyeyMhxKn4UpPJRNRqNaHruiiXy2IwGBzrKk75e0q7EIJcLsfT0xPVapXHx0cvNt5qu+M4lEolCoUCsVgMSTr6ap7Ud+/5j+r3fqNu8Bv80voDcNoKvQF9Ux4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from matplotlib import pyplot\n",
    "def split(word):\n",
    "    return [char for char in word]\n",
    "\n",
    "def generate_png(digits_string):\n",
    "    digit_list = split(digits_string)\n",
    "    n = len(digit_list)\n",
    "    print(\"n: \" + str(n))\n",
    "    count =0\n",
    "    for digit in digit_list:\n",
    "        count=count+1\n",
    "        model = load_model('/Users/paullintilhac/engs108-hw/'+digit+'_ld5_generator_model_005.h5', compile=False)\n",
    "        latent_points = generate_latent_points(5, 1)\n",
    "        X = model.predict(latent_points)\n",
    "        pyplot.subplot(n, 1, count)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(X[0, :, :, 0], cmap='gray_r')\n",
    "        pyplot.tight_layout()\n",
    "\n",
    "        pyplot.show()\n",
    "\n",
    "generate_png(\"0123456789\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fdf677",
   "metadata": {},
   "source": [
    "The code below uses models that were trained on 10 epochs instead of 5. You can see the images are more realistic. I did not have time to train all of the digits on this number of epochs, so you can only see the results for this n_epochs for digits 0,1, and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06702246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB1klEQVR4nO2WsariQBSGT5ZVI4gghJjCwkYhYJHWwi6laGkp+hC+gI2thY8QomivIGhlYWGhdjZpRDRIAqKoROe/xVZ6r9fNbti7cP1hqn843znDzDnDAaCv0o8vI7/gL/i/1s8nvhfvkHtkvI7dtc7nMw2HQzJNk06nEwWDQcpmsxSJRH4/CIDP1oeybRu1Wg2KoiCfzyORSCAUCqFcLmO9Xt9vfxjfFZwxBsMwkMvlIEkSOp0OLMvCZDKBqqrw+/1QVRX7/d57eKvVgiAIiMViqNfrYIwBAK7XK6rVKjiOA8/z0HXdW/hgMIAgCBBFEb1e7yYpx3FQqVRAREgmkzBN01u4pmmIRqPo9/v3B4LtdgtZlkFE0DTt3v57+Hg8RrvdxuVyeQefz+fw+XxIp9M4HA7ew3e7HabTKRzHuYm8XC5RLBbBcRyazea7xDyBA8BoNEK324VlWbBtG7quIx6Pg4ggSRIMw3AFd9VkGGNUKpWI53kiItpsNnQ8HimVSlEmk3HXYNw2mcViAVEUQb8GDsLhMBRFwWw2+6jip5Vz+PwDeWMCoNVqRY1GgxhjVCgUSJZlCgQCxHEPh9djww38D/V/jtRnF+5h1l7o+34mXvDvB38DWPOa14f/zYYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABQElEQVR4nO2WvaqDQBCFz8ripkihaCOp0mhr7cOIgq2dL+RrWKX1DSzyAIGAxkIwQZl0t7rqvWMwRXJgq1nm29mfMyuICO+S9jbyF/6Fby25EH/FOxRTgVWVr/UINrxtW+R5jqqq+HQimhu/6na7URzHtNvtKAxDGsdxairN5WdVXlUViqJA3/e4Xq94PB6swllw13Xh+z4AoO/7beGmaSJJEkgpUdc1mqbZDg4Ax+MRQggopaCU2hbedR2ICI7jwLbtbeGXywVCCARBACmXvOrFcMMwIKWE53ncFHz44XCApmk4nU7ouo6XhGMyRETn85mUUrTf7ynLMhqG4d8mwzssAMMwYBxH3O93lGXJ8nk23LIspGkKXdcRRRHr0omFFc8HiSDEZMf8YUwFVrXUP4BntbRX67Iv6HP/cF/458GfhIUeptsZhUAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAfCAYAAAAfrhY5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABrUlEQVR4nO1Vq47CUBSclm0JXExDQfH4AAwKC3/RINBA+Al+oAaHRyKqESQEh0CTEP6gPAUFmnZWbLJm6W7b7INkmaTmnuZMZu6ceySS+CvIf8b8JH+S/zZevqh/xxxKQYWHVh4I13VxPB6hKApSqRRUVY3ehORn3wd4nkfLsthsNlkoFJjP59nr9bjZbO79zs/6RyJ3HIeDwYC6rjOZTFKWZQKgEIL9fp++70cij2S7oigwDAOapmG322E6nWI2m8H3fSQSCUhSYLbuQvpisQQWScK2bZimCSEEut0udF2/yxHUI3bgJElCOp1GLpdDsVhENpuN3CP2qN1uN4zHY4xGo1iWA4gWON/3ud/vOZlMaBgGNU2jLMtst9s8HA4/l/bFYsFWq8V6vU4hBPGWBwKgpmk0TZOXy+X7075ardDpdLBcLt/PhBCo1Wool8twXRe2bWO9XqNSqYS/gjDKh8MhVVV9V9poNDifz7ndbnk+n3k6nWjbNh3HiaQ81Kh5ngfLsnC9XgEApVIJ1WoVmUwmjL5AG2LPeQQ85lZ76JUa4+UIj/9r+5P8/5G/ApSwJputbmcGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: /Users/paullintilhac/engs108-hw/3_ld5_generator_model_010.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-53a1e1584423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgenerate_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0123456789\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-53a1e1584423>\u001b[0m in \u001b[0;36mgenerate_png\u001b[0;34m(digits_string)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdigit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdigit_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/paullintilhac/engs108-hw/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdigit\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_ld5_generator_model_010.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlatent_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_latent_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   raise IOError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     raise IOError(\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /Users/paullintilhac/engs108-hw/3_ld5_generator_model_010.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from matplotlib import pyplot\n",
    "def split(word):\n",
    "    return [char for char in word]\n",
    "\n",
    "def generate_png(digits_string):\n",
    "    digit_list = split(digits_string)\n",
    "    n = len(digit_list)\n",
    "    print(\"n: \" + str(n))\n",
    "    count =0\n",
    "    for digit in digit_list:\n",
    "        count=count+1\n",
    "        model = load_model('/Users/paullintilhac/engs108-hw/'+digit+'_ld5_generator_model_010.h5', compile=False)\n",
    "        latent_points = generate_latent_points(5, 1)\n",
    "        X = model.predict(latent_points)\n",
    "        pyplot.subplot(n, 1, count)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(X[0, :, :, 0], cmap='gray_r')\n",
    "        pyplot.tight_layout()\n",
    "\n",
    "        pyplot.show()\n",
    "\n",
    "generate_png(\"012\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
